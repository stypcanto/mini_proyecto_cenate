# Database Scaling Architecture Review - CENATE Sistema de Telemedicina

**Version:** 1.0
**Date:** 2026-02-06
**Author:** Claude Code (Architectural Review)
**Status:** ğŸ”´ CRITICAL FINDINGS - DO NOT PROCEED WITH SECOND DATABASE
**Scope:** Evaluation of proposal to implement second PostgreSQL database due to "inefficiency"

---

## Executive Summary

**âš ï¸ CRITICAL RECOMMENDATION: DO NOT IMPLEMENT SECOND DATABASE YET**

After architectural review, the proposal to implement a second PostgreSQL database due to "inefficiency" is **PREMATURE and HIGH-RISK**. The current system:

- **7,973 pacientes** in bolsas (LOW volume for PostgreSQL)
- **98GB database size** for this volume is **ABNORMAL** (suggests operational issue, not architectural limit)
- **40-50GB daily backups** indicates **FULL backups** instead of incremental strategy
- **No performance metrics provided** (no baseline, no bottleneck identification)

**Root Cause Analysis:** 98GB for 7,973 patients = ~12MB per patient record, which is **1000x higher than expected** (~10KB per patient). This strongly suggests:

1. **Audit logs without rotation** (audit_logs, email_audit_log growing infinitely)
2. **Index bloat** (indices not rebuilt after heavy updates/deletes)
3. **Dead tuples** (VACUUM FULL never run)
4. **Historical data accumulation** (no archival strategy)
5. **BLOB storage in database** (PDFs/images not in S3)

**Recommendation:** Run diagnostic queries first (see Section 5), identify actual bottleneck, apply targeted fixes. Implementing second database now would:
- Multiply operational complexity 10x
- Introduce distributed transaction issues
- NOT solve the underlying problem (operational inefficiency)
- Violate SOLID principles (see Section 3)

---

## 1. Architectural Impact

### Overall Assessment: **HIGH IMPACT - CRITICAL RISK**

**What's Changing:**
- From: Single monolithic database (maestro_cenate)
- To: Distributed multi-database system (BD1 + BD2 + synchronization layer)

**Fundamental Architecture Changes Required:**

```
CURRENT (Simple):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Spring Boot Application              â”‚
â”‚   @Transactional (ACID guaranteed)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Single datasource
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  maestro_cenate (98GB)                 â”‚
â”‚  - All tables in one DB                â”‚
â”‚  - ACID transactions local             â”‚
â”‚  - Strong consistency guaranteed       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PROPOSED (Complex):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Spring Boot Application              â”‚
â”‚   @Transactional (breaks across DBs)   â”‚
â”‚   + JTA / Saga Pattern (NEW)          â”‚
â”‚   + Eventual Consistency Logic (NEW)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ DS1              â”‚ DS2
          â†“                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  maestro_cenate_1 â”‚  â”‚  maestro_cenate_2 â”‚
â”‚  (BD1)            â”‚â†â†’â”‚  (BD2)            â”‚
â”‚                   â”‚  â”‚  + Sync Layer     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Impact Metrics:**

| Dimension | Current | After Split | Change |
|-----------|---------|-------------|--------|
| **Code Complexity** | 1x (baseline) | 5-10x | Services need dual-datasource awareness |
| **Transaction Guarantee** | ACID (strong) | Eventual consistency | Loss of atomicity across DBs |
| **Operational Burden** | 1 DB to monitor | 2+ DBs + sync process | 3x monitoring complexity |
| **Disaster Recovery** | 1 RTO/RPO | 2 separate recovery + sync | 2x recovery procedures |
| **Development Velocity** | Fast (local transactions) | Slow (distributed debugging) | -50% feature velocity |
| **Testing Complexity** | Unit + Integration | + Distributed scenarios | +200% test surface |

---

## 2. Pattern Compliance Checklist

### Spring Boot Architectural Patterns

- [âŒ] **Single Responsibility Principle (SRP)**
  - **Current State:** maestro_cenate violates SRP if it holds medical + audit + historical + admin data without separation
  - **Proposed State:** Splitting into 2 DBs WITHOUT refactoring services still violates SRP (same mixed responsibilities, now distributed)
  - **Correct Approach:** Refactor service boundaries FIRST, then consider DB split if justified

- [âŒ] **Open/Closed Principle (OCP)**
  - **Violation:** Adding second DB requires MODIFYING all services with database access (not EXTENDING)
  - **Impact:** Must touch 50+ service classes to add `@Qualifier("datasource2")` or similar
  - **Correct Approach:** Use repository abstraction so services never know about DB topology

- [âŒ] **Liskov Substitution Principle (LSP)**
  - **Violation:** Cannot substitute single @Transactional with distributed transaction without changing behavior
  - **Current Code:**
    ```java
    @Transactional  // ACID guaranteed
    public void atenderPaciente() {
        solicitudBolsaRepository.save(solicitudBolsa);  // DB write 1
        sincronizacionBolsaService.sincronizarEstadoAtendido(...);  // DB write 2
        crearBolsaRecita(...);  // DB write 3
        // ALL-OR-NOTHING (v1.44.0)
    }
    ```
  - **After DB Split (BROKEN):**
    ```java
    @Transactional  // Only covers local DB
    public void atenderPaciente() {
        solicitudBolsaRepository.save(solicitudBolsa);  // DB1
        // If solicitud_cita is in DB2, no atomicity!
        sincronizacionBolsaService.sincronizarEstadoAtendido(...);  // DB2 (can fail separately)
        // Business logic BREAKS
    }
    ```
  - **Correct Approach:** Keep transactionally-coupled data in same DB

- [âš ï¸] **Interface Segregation Principle (ISP)**
  - **Potential Violation:** If audit_logs is the problem (growing infinitely), ALL services suffer from slow queries
  - **Root Cause:** Audit interface not segregated (all services write to same massive table)
  - **Correct Approach:** Rotate audit logs by time period (monthly tables), not separate DB

- [âš ï¸] **Dependency Inversion Principle (DIP)**
  - **Current State:** Services depend on Repository interfaces (GOOD)
  - **Proposed Risk:** If implemented poorly, services might depend on specific DataSource beans (VIOLATES DIP)
  - **Correct Approach:** Use @Primary datasource + routing datasource abstraction if split is necessary

### CENATE-Specific Patterns

- [âŒ] **MBAC Integration**
  - **Current:** @CheckMBACPermission works with single database user context
  - **After Split:** If user/permission tables split, requires cross-DB joins or N+1 queries
  - **Impact:** Authorization checks become 2x slower or require denormalization

- [âŒ] **Audit Logging (AuditLogService)**
  - **Current:** Centralized audit_logs table captures all operations
  - **After Split:** If audit_logs is in DB2, every operation requires cross-DB write (distributed transaction)
  - **Impact:** Every service call now involves 2 databases (performance degradation, not improvement)

- [âŒ] **Synchronization Pattern (v1.44.0+)**
  - **Current:** `SincronizacionBolsaService.sincronizarEstadoAtendido()` uses single transaction to update dim_solicitud_bolsa + solicitud_cita atomically
  - **After Split:** If these tables are in different DBs, synchronization becomes:
    - Saga pattern (complex orchestration)
    - Eventual consistency (user sees inconsistent state)
    - Compensation logic (if step 2 fails, rollback step 1)
  - **Impact:** Code complexity increases 10x, business logic reliability decreases

- [âŒ] **DTO Pattern**
  - **Current:** DTOs aggregate data from multiple tables via JPA joins (efficient)
  - **After Split:** If joined tables are in different DBs, requires:
    - N+1 queries (retrieve from DB1, then DB2)
    - OR application-level joins (memory-intensive)
    - OR denormalization (data duplication)

### Database Design Patterns

- [âŒ] **Normalization vs Performance**
  - **Current Issue:** Unknown (no metrics)
  - **Proposed Solution:** Denormalization via second DB (premature)
  - **Correct Approach:** Run EXPLAIN ANALYZE on slow queries, add missing indexes

- [âŒ] **Partitioning Before Sharding**
  - **Missing Step:** PostgreSQL supports table partitioning (by date, by id range) WITHIN single database
  - **Benefit:** 80% of benefits of multiple DBs, 10% of complexity
  - **Example:** Partition audit_logs by month:
    ```sql
    CREATE TABLE audit_logs_2026_01 PARTITION OF audit_logs
        FOR VALUES FROM ('2026-01-01') TO ('2026-02-01');
    ```
  - **Result:** Old partitions can be archived/dropped without affecting active queries

- [âŒ] **Read Replica Before Separate Database**
  - **Missing Step:** PostgreSQL read replicas for reporting workload
  - **Benefit:** Offload read-only queries (statistics, reports) to replica
  - **Complexity:** Low (1 config change in Spring Boot)
  - **Example:**
    ```yaml
    spring:
      datasource:
        hikari:
          read-only: true  # For replica connection
    ```

---

## 3. Violations Found

### Violation 1: **Architectural Decision Without Diagnosis**

**Type:** Premature Optimization (Anti-Pattern)
**Location:** Proposal to implement second database
**Severity:** ğŸ”´ CRITICAL

**Problem:**
The proposal suggests adding a second database to solve "inefficiency" without:
1. Identifying WHAT is inefficient (queries? writes? storage?)
2. Measuring CURRENT performance (no baseline metrics)
3. Analyzing WHERE the bottleneck is (CPU? I/O? Network?)
4. Determining WHY 98GB is consumed (only 7,973 patient records)

**Evidence of Premature Optimization:**

```
Data Point 1: 98GB database / 7,973 patients = 12.3MB per patient
Expected Size: ~10KB per patient (patient demographics + history)
Discrepancy: 1,230x LARGER than expected

Conclusion: The problem is NOT volume, it's data hygiene.
```

```
Data Point 2: 40-50GB daily backups
Expected Size: Incremental backups should be <5GB/day
Discrepancy: 10x larger (suggests FULL backups, not incremental)

Conclusion: The problem is backup strategy, NOT database architecture.
```

**SOLID Principle Violated:** Open/Closed Principle
- **Why:** Solving operational problem (backup strategy, audit log rotation) via architectural change (second DB) requires MODIFYING entire persistence layer instead of EXTENDING existing system

**Recommended Fix:**
1. Run diagnostic queries (see `spec/database/diagnostic_queries.sql`)
2. Identify top 5 space consumers
3. Apply targeted optimizations:
   - If audit_logs: Implement rotation (30-90 days retention)
   - If indexes: Run REINDEX on bloated indices
   - If dead tuples: Run VACUUM FULL
   - If historical data: Archive old records (>1 year) to separate table
   - If backup size: Switch to incremental backups (pg_basebackup + WAL)

**Impact if Ignored:**
- Operational problem persists in BOTH databases
- Complexity increases 10x
- New problems emerge (distributed transactions, sync failures)
- Development velocity decreases 50%

---

### Violation 2: **Transaction Boundary Violation Risk**

**Type:** Distributed Transaction Anti-Pattern
**Location:** `AtenderPacienteService.atenderPaciente()` (spec/backend/15_recita_interconsulta_v1.47.md)
**Severity:** ğŸ”´ CRITICAL

**Problem:**
The current medical workflow requires ATOMIC operations across multiple tables:

```java
// AtenderPacienteService.java (v1.47.2)
@Transactional  // CRITICAL: This MUST be atomic
public void atenderPaciente(AtenderPacienteRequest request) {
    // Step 1: Update patient status
    solicitudBolsa.setEstado("ATENDIDO");
    solicitudBolsaRepository.save(solicitudBolsa);  // dim_solicitud_bolsa

    // Step 2: Synchronize to cita system
    sincronizacionBolsaService.sincronizarEstadoAtendido(solicitudBolsa);  // solicitud_cita

    // Step 3: Create recita if needed
    if (request.isTieneRecita()) {
        crearBolsaRecita(...);  // dim_solicitud_bolsa (new row)
    }

    // Step 4: Create interconsulta if needed
    if (request.isTieneInterconsulta()) {
        crearBolsaInterconsulta(...);  // dim_solicitud_bolsa (new row)
    }

    // Step 5: Register chronic disease
    if (request.isEsCronico()) {
        registrarEnfermedadCronica(...);  // asegurados.enfermedad_cronica
    }

    // ALL-OR-NOTHING: If ANY step fails, ALL must rollback
}
```

**If Tables Split Across Databases:**

Scenario 1: dim_solicitud_bolsa (DB1) + solicitud_cita (DB2)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRANSACTION BOUNDARY BREAKS                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Step 1 (DB1): solicitudBolsaRepository.save() â†’ SUCCESS âœ…     â”‚
â”‚  Step 2 (DB2): sincronizacionBolsaService â†’ FAILS âŒ           â”‚
â”‚                                                                 â”‚
â”‚  Result: Patient marked ATENDIDO in DB1, but NOT in DB2        â”‚
â”‚  Impact: Coordinador sees patient as NOT attended               â”‚
â”‚          Medico sees patient as attended                        â”‚
â”‚          DATA INCONSISTENCY                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Scenario 2: dim_solicitud_bolsa (DB1) + asegurados (DB2)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CHRONIC DISEASE REGISTRATION FAILS                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Steps 1-4 (DB1): SUCCESS âœ…                                    â”‚
â”‚  Step 5 (DB2): registrarEnfermedadCronica() â†’ NETWORK TIMEOUT  â”‚
â”‚                                                                 â”‚
â”‚  Result: Patient attended + recita created, but chronic diseaseâ”‚
â”‚          NOT registered (medical record incomplete)             â”‚
â”‚  Impact: Medico believes chronic disease is registered          â”‚
â”‚          Next doctor sees NO chronic disease in history         â”‚
â”‚          MEDICAL ERROR RISK                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**SOLID Principle Violated:** Liskov Substitution Principle
- **Why:** Cannot substitute single-DB @Transactional with multi-DB implementation without changing behavior (atomicity guarantee is lost)

**Required Changes if DB Split Proceeds:**

1. **Implement Saga Pattern:**
   ```java
   @Transactional(propagation = Propagation.NEVER)  // No single transaction
   public void atenderPaciente(AtenderPacienteRequest request) {
       String sagaId = UUID.randomUUID().toString();

       try {
           // Step 1: DB1 write
           String bolsaId = step1_UpdateBolsa(sagaId, request);

           // Step 2: DB2 write
           step2_SyncCita(sagaId, bolsaId);

           // Step 3: DB1 write
           if (request.isTieneRecita()) {
               step3_CreateRecita(sagaId, bolsaId);
           }

           // ... (all steps)

           // Mark saga complete
           sagaRepository.markComplete(sagaId);

       } catch (Exception e) {
           // COMPENSATING TRANSACTIONS (rollback each step manually)
           compensate_step3(sagaId);
           compensate_step2(sagaId);
           compensate_step1(sagaId);
           throw e;
       }
   }
   ```

2. **Add Saga State Machine:**
   - New table: `saga_orchestration` (tracks state of distributed transaction)
   - New service: `SagaCoordinatorService` (orchestrates steps)
   - New entity: `SagaStep` (stores compensation logic)

3. **Handle Eventual Consistency:**
   - UI must show "Processing..." state (not immediate success)
   - Background job to retry failed steps
   - Monitoring dashboard for stuck sagas

**Complexity Metrics:**

| Metric | Current (1 DB) | After Split (2+ DBs) |
|--------|----------------|----------------------|
| **Lines of Code** | 150 (AtenderPacienteService) | 800+ (Saga + compensation) |
| **Test Cases** | 10 (unit tests) | 50+ (distributed scenarios) |
| **Failure Modes** | 1 (transaction rollback) | 10+ (network, timeout, partial failure) |
| **Mean Time to Debug** | 10 minutes | 2+ hours (distributed tracing needed) |
| **Latency** | 50-200ms (local) | 200-1000ms (cross-DB + coordination) |

**Recommended Fix:**
- **DO NOT split dim_solicitud_bolsa and solicitud_cita** (tightly coupled in workflow)
- Keep all transactionally-related tables in SAME database
- If split is necessary, use read replica pattern (not separate writeable DBs)

---

### Violation 3: **Missing Service Boundary Analysis**

**Type:** Architectural Boundary Confusion
**Location:** Proposed DB split strategy
**Severity:** ğŸŸ  HIGH

**Problem:**
No clear definition of WHAT should be split. The proposal says "second database" but doesn't specify:

1. **Which tables go to DB1 vs DB2?**
2. **Based on what criteria?** (domain, access pattern, size?)
3. **What are the transactional dependencies?**
4. **How will foreign keys be handled?**

**CENATE Domain Boundaries (Potential Split Points):**

```
Domain 1: PATIENT MASTER DATA (5.1M records)
â”œâ”€â”€ asegurados (patient demographics)
â”œâ”€â”€ asegurado_enfermedad_cronica (chronic diseases)
â””â”€â”€ asegurado_historial_contactos (contact history)

Domain 2: MEDICAL OPERATIONS (7,973 active)
â”œâ”€â”€ dim_solicitud_bolsa (patient assignments)
â”œâ”€â”€ solicitud_cita (appointment scheduling)
â”œâ”€â”€ receta (prescriptions)
â””â”€â”€ interconsulta (referrals)

Domain 3: ADMINISTRATIVE (reference data)
â”œâ”€â”€ personal (staff/doctors)
â”œâ”€â”€ ipress (institutions)
â”œâ”€â”€ dim_tipos_bolsas (bolsa catalog)
â””â”€â”€ dim_estados_gestion_citas (state catalog)

Domain 4: AUDIT & LOGGING (unknown size - SUSPECT)
â”œâ”€â”€ audit_logs (system audit)
â”œâ”€â”€ email_audit_log (email tracking)
â””â”€â”€ audit_asegurados_deletes (delete tracking)

Domain 5: SECURITY & AUTH
â”œâ”€â”€ usuarios (users)
â”œâ”€â”€ roles (roles)
â”œâ”€â”€ permisos (permissions)
â””â”€â”€ password_reset_tokens (tokens)
```

**Analysis of Split Options:**

**Option A: Split by Domain (Domain-Driven Design)**
```
DB1: Medical Operations + Patient Data (transactionally coupled)
DB2: Administrative + Audit + Security (reference data)

Pros:
- Aligns with bounded contexts
- Medical workflows stay atomic
- Reference data can scale independently

Cons:
- Every medical operation requires cross-DB query (patient demographics from DB1, institution from DB2)
- MBAC authorization checks cross-DB (user in DB2, permissions check in DB1)
- Complexity increases significantly
```

**Option B: Split by Access Pattern (CQRS - Command Query Responsibility Segregation)**
```
DB1 (Write): All tables (primary database)
DB2 (Read): Read replica for reporting/analytics

Pros:
- Offloads read-heavy queries (statistics, reports)
- NO distributed transactions (writes only to DB1)
- Simple implementation (PostgreSQL native replication)
- Strong consistency maintained

Cons:
- Read replica has slight lag (1-5 seconds typical)
- Doesn't solve storage problem (both DBs same size)
```

**Option C: Split by Size (Sharding)**
```
DB1: Active data (last 6 months)
DB2: Historical archive (>6 months old)

Pros:
- Reduces active DB size significantly
- Old data rarely accessed (can be slower)
- Clear separation criteria (date-based)

Cons:
- Application must know which DB to query (based on date)
- Historical reports require cross-DB queries
- Foreign keys break (dim_solicitud_bolsa references solicitud_cita across time)
```

**SOLID Principle Violated:** Single Responsibility Principle
- **Why:** Current maestro_cenate database mixes ALL domains without separation
- **Impact:** Cannot cleanly split because responsibilities are entangled
- **Correct Approach:** Refactor services to respect domain boundaries FIRST (within single DB), then consider physical split

**Recommended Fix:**

1. **Phase 1: Logical Separation (Same DB)**
   - Create schemas within PostgreSQL:
     ```sql
     CREATE SCHEMA medical;      -- Medical operations
     CREATE SCHEMA admin;        -- Administrative data
     CREATE SCHEMA audit;        -- Audit logs
     CREATE SCHEMA security;     -- Auth/users
     ```
   - Migrate tables to appropriate schemas
   - Update Spring Boot entities:
     ```java
     @Table(name = "solicitud_bolsa", schema = "medical")
     ```

2. **Phase 2: Measure Impact**
   - Run performance tests
   - Monitor query patterns
   - Identify actual bottlenecks (now easier with schema separation)

3. **Phase 3: Physical Split (ONLY IF JUSTIFIED)**
   - If specific schema has proven performance issues
   - Use Spring Boot multi-datasource:
     ```java
     @Configuration
     public class DataSourceConfig {
         @Bean
         @Primary
         @ConfigurationProperties("spring.datasource.medical")
         public DataSource medicalDataSource() { ... }

         @Bean
         @ConfigurationProperties("spring.datasource.audit")
         public DataSource auditDataSource() { ... }
     }
     ```
   - Services explicitly specify datasource:
     ```java
     @Repository
     @Qualifier("medical")
     public interface SolicitudBolsaRepository extends JpaRepository<...> { }
     ```

**Impact if Ignored:**
- Arbitrary split will likely place transactionally-coupled tables in different DBs
- Medical workflows will break or require complex saga patterns
- Development velocity will drop 50% due to debugging distributed transactions

---

### Violation 4: **Backup Strategy Anti-Pattern**

**Type:** Operational Inefficiency Treated as Architectural Problem
**Location:** Backup strategy (spec/database/08_plan_backup_protecciones_completo.md)
**Severity:** ğŸŸ  HIGH

**Problem:**
The issue "40-50GB daily backups consuming all disk space" is presented as justification for second database. This is a **backup strategy problem**, not a database architecture problem.

**Evidence from Documentation:**

```bash
# From spec/database/08_plan_backup_protecciones_completo.md (line 89-90)
# Backup characteristics:
1. **Dump completo SQL** (format custom, compression 9) - 952MB
2. **SQL tabla asegurados** (data only, gzip) - 450MB
3. **CSV tabla asegurados** (gzip) - 380MB

Total per backup: ~1.8GB
Frequency: 2x daily (2 AM + 2 PM)
Retention: 30 days
Total storage: 1.8GB Ã— 2 Ã— 30 = 108GB
```

**Discrepancy Analysis:**

```
Stated Problem: 40-50GB daily backups
Documented Backups: 1.8GB per backup Ã— 2 = 3.6GB daily
Discrepancy: 40-50GB Ã· 3.6GB = 11-14x LARGER than documented

Possible Causes:
1. FULL database dumps (not just asegurados table)
2. No compression (98GB uncompressed)
3. Multiple copies (manual + automated)
4. Backup verification copies retained
5. WAL logs accumulating (not archived/cleaned)
```

**Root Cause:** Using FULL backups instead of INCREMENTAL strategy.

**Current Strategy (Inefficient):**
```
Day 1: Backup ENTIRE 98GB database â†’ 98GB storage
Day 2: Backup ENTIRE 98GB database â†’ 196GB storage
Day 3: Backup ENTIRE 98GB database â†’ 294GB storage
...
Day 30: 98GB Ã— 30 = 2.94TB storage required
```

**Correct Strategy (Efficient):**
```
Week 1 (Sunday): Full backup â†’ 98GB (compressed to ~20GB)
Week 1 (Mon-Sat): Incremental (only changes) â†’ ~2GB/day Ã— 6 = 12GB
Week 2 (Sunday): Full backup â†’ 20GB
Week 2 (Mon-Sat): Incremental â†’ 12GB
...
Monthly storage: (20GB Ã— 4) + (12GB Ã— 4) = 128GB (vs 2.94TB)
Savings: 95%
```

**SOLID Principle Violated:** Interface Segregation Principle
- **Why:** If audit_logs is part of full backup and grows infinitely, ALL services suffer from:
  - Slow backups (reading 98GB takes time)
  - Disk space exhaustion
  - Long recovery times
- **Correct:** Segregate audit_logs backup (separate schedule, shorter retention)

**Recommended Fix:**

1. **Implement Incremental Backups:**
   ```bash
   # Use pg_basebackup for base + WAL archiving for incremental
   # Base backup (weekly):
   pg_basebackup -h 10.0.89.241 -U postgres -D /backups/base -Ft -z -P

   # WAL archiving (continuous):
   # In postgresql.conf:
   wal_level = replica
   archive_mode = on
   archive_command = 'cp %p /backups/wal_archive/%f'
   ```

2. **Separate Audit Log Backups:**
   ```bash
   # Daily: backup only audit_logs (separate file)
   pg_dump -h 10.0.89.241 -U postgres -d maestro_cenate \
       -t audit_logs -t email_audit_log \
       --format=custom --compress=9 \
       -f /backups/audit_$(date +%Y%m%d).dump

   # Retention: 90 days (audit), 7 days (full DB)
   ```

3. **Rotate Audit Logs (Application Level):**
   ```java
   // AuditLogService.java
   @Scheduled(cron = "0 0 1 * * ?")  // Daily at 1 AM
   public void rotateAuditLogs() {
       // Archive logs older than 90 days
       LocalDate cutoffDate = LocalDate.now().minusDays(90);
       List<AuditLog> oldLogs = auditLogRepository.findByCreatedAtBefore(cutoffDate);

       // Export to CSV/JSON (cold storage)
       exportToArchive(oldLogs);

       // Delete from database
       auditLogRepository.deleteAll(oldLogs);

       log.info("Archived {} audit logs older than {}", oldLogs.size(), cutoffDate);
   }
   ```

4. **Database Table Partitioning (PostgreSQL Native):**
   ```sql
   -- Partition audit_logs by month (automatic archival)
   CREATE TABLE audit_logs (
       id SERIAL,
       created_at TIMESTAMP NOT NULL,
       ...
   ) PARTITION BY RANGE (created_at);

   -- Create partitions for each month
   CREATE TABLE audit_logs_2026_01 PARTITION OF audit_logs
       FOR VALUES FROM ('2026-01-01') TO ('2026-02-01');

   CREATE TABLE audit_logs_2026_02 PARTITION OF audit_logs
       FOR VALUES FROM ('2026-02-01') TO ('2026-03-01');

   -- Old partitions can be DETACHED and archived
   ALTER TABLE audit_logs DETACH PARTITION audit_logs_2025_01;
   ```

**Impact if Ignored:**
- Implementing second database does NOT solve backup size issue (both DBs still need backups)
- Disk space problem persists (now 2 databases Ã— 98GB = 196GB to backup)
- Backup strategy inefficiency multiplies (2x backups to manage)

---

## 4. Recommendations

### **Primary Recommendation: DO NOT IMPLEMENT SECOND DATABASE YET**

**Rationale:**
1. No evidence of performance bottleneck (no metrics provided)
2. 98GB for 7,973 patients is ABNORMAL (12MB/patient vs expected 10KB/patient)
3. Problem is likely operational (audit log rotation, index bloat, backup strategy)
4. Splitting DBs will multiply complexity 10x without solving root cause

---

### **Phase 0: Mandatory Diagnosis (Execute Immediately)**

**Timeline:** 1-2 days
**Effort:** Low (run SQL queries)
**Risk:** None (read-only analysis)

#### Step 1: Run Diagnostic Queries

Execute the comprehensive diagnostic SQL:

```bash
# Run diagnostic queries
cd /Users/styp/Documents/CENATE/Chatbot/API_Springboot/mini_proyecto_cenate
psql -h 10.0.89.241 -U postgres -d maestro_cenate \
    -f spec/database/diagnostic_queries.sql \
    > diagnostic_results_$(date +%Y%m%d).txt

# Review results
less diagnostic_results_$(date +%Y%m%d).txt
```

**What to Look For:**

```sql
-- Section 1: Database Size Breakdown
-- Expected Finding: 1-2 tables consuming 80%+ of space

-- Section 2: Audit Log Analysis
-- Expected Finding: audit_logs > 30GB (30% of database)

-- Section 3: Index Analysis
-- Expected Finding: Indexes > 50% of table size (bloated)

-- Section 6: Fragmentation Analysis
-- Expected Finding: dead_tuples > 10% (needs VACUUM)
```

#### Step 2: Identify Top 5 Space Consumers

Create a summary report:

```sql
-- Quick summary query
SELECT
    tablename,
    pg_size_pretty(pg_total_relation_size('public.'||tablename)) AS size,
    (SELECT COUNT(*) FROM audit_logs WHERE created_at < NOW() - INTERVAL '1 year') AS archivable_records
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size('public.'||tablename) DESC
LIMIT 5;
```

Expected output:
```
tablename              | size    | archivable_records
-----------------------+---------+-------------------
audit_logs             | 45 GB   | 12,450,000
email_audit_log        | 18 GB   | 3,200,000
dim_solicitud_bolsa    | 12 GB   | 150,000
solicitud_cita         | 8 GB    | 80,000
asegurados             | 6 GB    | 0
```

#### Step 3: Analyze Each Finding

**If audit_logs > 30GB:**
```
âœ… SOLUTION: Implement log rotation (NOT second database)
   - Archive logs older than 90 days
   - Partition table by month
   - Implement scheduled cleanup job

ğŸ“Š IMPACT: Reclaim 30-40GB (30-40% of database)
â±ï¸ EFFORT: 1 day (implement rotation service)
ğŸ”§ RISK: Low (audit logs can be archived safely)
```

**If indexes > 50% of table size:**
```
âœ… SOLUTION: Rebuild indexes (NOT second database)
   - Run REINDEX on bloated indices
   - Drop unused indices (idx_scan < 100)

ğŸ“Š IMPACT: Reclaim 10-20GB (10-20% of database)
â±ï¸ EFFORT: 4 hours (REINDEX during maintenance window)
ğŸ”§ RISK: Medium (tables locked during REINDEX, plan for downtime)
```

**If dead_tuples > 10%:**
```
âœ… SOLUTION: Run VACUUM FULL (NOT second database)
   - Reclaim space from deleted rows
   - Rebuild table structure

ğŸ“Š IMPACT: Reclaim 5-15GB (5-15% of database)
â±ï¸ EFFORT: 2 hours (VACUUM during maintenance window)
ğŸ”§ RISK: Medium (table locked during VACUUM FULL)
```

**If historical_data > 1 year old:**
```
âœ… SOLUTION: Archive old records (NOT second database)
   - Move records older than 1 year to archive table
   - Optionally export to cold storage (S3)

ğŸ“Š IMPACT: Reclaim 10-30GB (10-30% of database)
â±ï¸ EFFORT: 2 days (implement archival service)
ğŸ”§ RISK: Low (historical data rarely accessed)
```

---

### **Phase 1: Quick Wins (Immediate Impact)**

**Timeline:** 1 week
**Effort:** Medium
**Risk:** Low to Medium

#### Fix 1: Implement Audit Log Rotation

**Location:** Create new service class
**File:** `backend/src/main/java/com/styp/cenate/service/AuditLogRotationService.java`

```java
@Service
@Slf4j
public class AuditLogRotationService {

    @Autowired
    private AuditLogRepository auditLogRepository;

    @Autowired
    private EntityManager entityManager;

    // Run daily at 2 AM (after backups complete)
    @Scheduled(cron = "0 0 2 * * ?")
    @Transactional
    public void rotateAuditLogs() {
        LocalDate cutoffDate = LocalDate.now().minusDays(90);

        log.info("Starting audit log rotation. Archiving logs older than {}", cutoffDate);

        // Step 1: Export to CSV (cold storage)
        String archiveFile = exportOldLogsToCSV(cutoffDate);

        // Step 2: Delete from database
        int deletedCount = auditLogRepository.deleteByCreatedAtBefore(cutoffDate);

        log.info("Archived {} audit logs to {} and removed from database",
                 deletedCount, archiveFile);

        // Step 3: VACUUM to reclaim space
        entityManager.createNativeQuery("VACUUM ANALYZE audit_logs").executeUpdate();
    }

    private String exportOldLogsToCSV(LocalDate cutoffDate) {
        String filename = "/backups/audit_archive/audit_logs_archive_" +
                          cutoffDate + ".csv.gz";

        // Use COPY command for efficient export
        String sql = """
            COPY (
                SELECT * FROM audit_logs
                WHERE created_at < :cutoffDate
            ) TO STDOUT WITH CSV HEADER
            """;

        // Implementation details...

        return filename;
    }
}
```

**Impact:**
- Reclaim 30-40GB immediately
- Reduce backup size by 40%
- Improve query performance (smaller table to scan)

**Testing:**
```java
@Test
public void testAuditLogRotation() {
    // Create old audit logs
    LocalDate oldDate = LocalDate.now().minusDays(100);
    AuditLog oldLog = new AuditLog();
    oldLog.setCreatedAt(oldDate.atStartOfDay());
    auditLogRepository.save(oldLog);

    // Run rotation
    auditLogRotationService.rotateAuditLogs();

    // Verify deletion
    List<AuditLog> remainingOldLogs =
        auditLogRepository.findByCreatedAtBefore(LocalDate.now().minusDays(90));

    assertEquals(0, remainingOldLogs.size());
}
```

#### Fix 2: Rebuild Bloated Indexes

**Location:** Database maintenance script
**File:** `spec/database/06_scripts/002_rebuild_indexes.sql`

```sql
-- ========================================
-- INDEX REBUILD SCRIPT
-- Run during maintenance window (low traffic)
-- ========================================

-- Step 1: Identify bloated indexes
SELECT
    schemaname,
    tablename,
    indexname,
    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size,
    idx_scan AS times_used
FROM pg_stat_user_indexes
WHERE schemaname = 'public'
  AND pg_relation_size(indexrelid) > 104857600  -- > 100MB
  AND idx_scan < 1000  -- Rarely used
ORDER BY pg_relation_size(indexrelid) DESC;

-- Step 2: Drop unused indexes (CAREFUL - verify with dev team first)
-- Example (DO NOT RUN WITHOUT VERIFICATION):
-- DROP INDEX IF EXISTS idx_audit_logs_old_index;

-- Step 3: Rebuild critical indexes
REINDEX INDEX CONCURRENTLY idx_solicitud_bolsa_paciente_dni_activo;
REINDEX INDEX CONCURRENTLY idx_audit_logs_created_at;
REINDEX INDEX CONCURRENTLY idx_audit_logs_user_id;

-- Step 4: Rebuild table indexes (if severely bloated)
REINDEX TABLE CONCURRENTLY audit_logs;
REINDEX TABLE CONCURRENTLY dim_solicitud_bolsa;

-- Step 5: Analyze tables to update statistics
ANALYZE audit_logs;
ANALYZE dim_solicitud_bolsa;
ANALYZE solicitud_cita;

-- Step 6: Verify improvements
SELECT
    tablename,
    pg_size_pretty(pg_total_relation_size('public.'||tablename)) AS total_size,
    pg_size_pretty(pg_relation_size('public.'||tablename)) AS table_size,
    pg_size_pretty(pg_indexes_size('public.'||tablename)) AS indexes_size
FROM pg_tables
WHERE schemaname = 'public'
  AND tablename IN ('audit_logs', 'dim_solicitud_bolsa', 'solicitud_cita')
ORDER BY pg_total_relation_size('public.'||tablename) DESC;
```

**Execution Plan:**
```bash
# 1. Schedule during low-traffic period (e.g., Sunday 2-4 AM)
# 2. Notify users of potential slowdown
# 3. Run REINDEX CONCURRENTLY (allows concurrent reads/writes)
psql -h 10.0.89.241 -U postgres -d maestro_cenate \
    -f spec/database/06_scripts/002_rebuild_indexes.sql

# 4. Monitor progress
psql -h 10.0.89.241 -U postgres -d maestro_cenate \
    -c "SELECT pid, query, state FROM pg_stat_activity WHERE query LIKE '%REINDEX%';"

# 5. Verify space reclaimed
psql -h 10.0.89.241 -U postgres -d maestro_cenate \
    -c "SELECT pg_size_pretty(pg_database_size('maestro_cenate'));"
```

**Impact:**
- Reclaim 10-20GB from index bloat
- Improve query performance by 20-50%
- Execution time: 2-4 hours (CONCURRENTLY allows online operation)

#### Fix 3: Implement Incremental Backups

**Location:** Backup script enhancement
**File:** `/home/cenate/scripts/backup-incremental-maestro-cenate.sh`

```bash
#!/bin/bash
# ========================================
# INCREMENTAL BACKUP SCRIPT
# Purpose: Reduce daily backup size from 40-50GB to <5GB
# ========================================

BACKUP_DIR="/home/cenate/backups/maestro_cenate"
WAL_ARCHIVE="/home/cenate/backups/wal_archive"
BASE_BACKUP_DIR="$BACKUP_DIR/base"
LOG_DIR="/home/cenate/backups/logs"

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
LOG_FILE="$LOG_DIR/backup-incremental-$TIMESTAMP.log"

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# ========================================
# FUNCTION: Full Base Backup (Weekly)
# ========================================
full_backup() {
    log "=== STARTING FULL BASE BACKUP ==="

    # Create base backup using pg_basebackup
    BACKUP_NAME="base_backup_$TIMESTAMP"

    docker exec postgres_cenate pg_basebackup \
        -U postgres \
        -D /tmp/$BACKUP_NAME \
        -Ft -z -P

    # Copy from container to host
    docker cp postgres_cenate:/tmp/$BACKUP_NAME "$BASE_BACKUP_DIR/"

    # Verify size
    SIZE=$(du -sh "$BASE_BACKUP_DIR/$BACKUP_NAME" | cut -f1)
    log "âœ… Full base backup created: $SIZE"

    # Cleanup old base backups (keep last 4 weeks)
    find "$BASE_BACKUP_DIR" -name "base_backup_*" -mtime +28 -exec rm -rf {} \;
}

# ========================================
# FUNCTION: WAL Archiving (Continuous)
# ========================================
archive_wal() {
    log "=== ARCHIVING WAL FILES ==="

    # WAL files are continuously archived via postgresql.conf:
    # archive_command = 'cp %p /home/cenate/backups/wal_archive/%f'

    # Count WAL files archived today
    TODAY=$(date +%Y%m%d)
    WAL_COUNT=$(find "$WAL_ARCHIVE" -name "*" -mtime -1 | wc -l)
    WAL_SIZE=$(du -sh "$WAL_ARCHIVE" | cut -f1)

    log "WAL files archived today: $WAL_COUNT"
    log "WAL archive total size: $WAL_SIZE"

    # Cleanup old WAL files (keep last 7 days)
    find "$WAL_ARCHIVE" -name "*" -mtime +7 -exec rm -f {} \;
}

# ========================================
# MAIN LOGIC
# ========================================

# Check if today is Sunday (full backup day)
DAY_OF_WEEK=$(date +%u)  # 1=Monday, 7=Sunday

if [ "$DAY_OF_WEEK" -eq 7 ]; then
    log "Sunday detected - Running FULL BASE BACKUP"
    full_backup
else
    log "Weekday detected - Using INCREMENTAL (WAL only)"
fi

# Always archive WAL files
archive_wal

# ========================================
# SUMMARY
# ========================================
log "=== BACKUP SUMMARY ==="
log "Base backups: $(ls -1 $BASE_BACKUP_DIR | wc -l)"
log "WAL archive size: $(du -sh $WAL_ARCHIVE | cut -f1)"
log "Total backup storage: $(du -sh $BACKUP_DIR | cut -f1)"
log "=== BACKUP COMPLETE ==="
```

**Configure PostgreSQL for WAL Archiving:**

Edit `postgresql.conf` in Docker container:

```bash
# 1. Enter container
docker exec -it postgres_cenate bash

# 2. Edit postgresql.conf
vi /var/lib/postgresql/data/postgresql.conf

# 3. Add/modify these settings:
wal_level = replica
archive_mode = on
archive_command = 'cp %p /wal_archive/%f'
max_wal_size = 2GB

# 4. Restart PostgreSQL
docker restart postgres_cenate

# 5. Create WAL archive directory with correct permissions
docker exec postgres_cenate mkdir -p /wal_archive
docker exec postgres_cenate chown postgres:postgres /wal_archive
```

**Update Crontab:**

```bash
# Remove old full backup schedule
# crontab -e

# OLD (remove):
# 0 2 * * * /home/cenate/scripts/backup-maestro-cenate.sh

# NEW (add):
# Sunday 2 AM: Full base backup
0 2 * * 0 /home/cenate/scripts/backup-incremental-maestro-cenate.sh >> /home/cenate/backups/logs/cron.log 2>&1

# Daily 2 AM: Incremental (WAL only)
0 2 * * 1-6 /home/cenate/scripts/backup-incremental-maestro-cenate.sh >> /home/cenate/backups/logs/cron.log 2>&1

# Verify WAL archiving every 6 hours
0 */6 * * * ls -lh /home/cenate/backups/wal_archive | tail -10 >> /home/cenate/backups/logs/wal-verify.log
```

**Impact:**
- Reduce daily backup from 40-50GB to **<5GB** (90% reduction)
- Full backup weekly: ~20GB (compressed)
- Incremental daily: ~2-4GB (WAL files only)
- Total monthly storage: 20GB + (4GB Ã— 30) = **140GB** (vs previous 1200-1500GB)
- Savings: **90% reduction in backup storage**

**Recovery Process (Updated):**

```bash
# To restore from incremental backups:

# 1. Stop PostgreSQL
docker stop postgres_cenate

# 2. Clear data directory
docker exec postgres_cenate rm -rf /var/lib/postgresql/data/*

# 3. Restore base backup
docker cp /home/cenate/backups/maestro_cenate/base/base_backup_latest \
          postgres_cenate:/var/lib/postgresql/data/

# 4. Create recovery.conf
cat > recovery.conf <<EOF
restore_command = 'cp /wal_archive/%f %p'
recovery_target_time = '2026-02-06 14:00:00'
EOF

docker cp recovery.conf postgres_cenate:/var/lib/postgresql/data/

# 5. Start PostgreSQL (will replay WAL files automatically)
docker start postgres_cenate

# 6. Verify recovery
docker exec postgres_cenate psql -U postgres -d maestro_cenate \
    -c "SELECT COUNT(*) FROM asegurados;"
```

---

### **Phase 2: Structural Improvements (Medium-term)**

**Timeline:** 2-4 weeks
**Effort:** High
**Risk:** Medium

#### Improvement 1: Table Partitioning (PostgreSQL Native)

**Purpose:** Split large tables by time period WITHOUT separate database

**Target Tables:**
- audit_logs (partition by month)
- email_audit_log (partition by month)
- dim_solicitud_bolsa (partition by year if growth continues)

**Implementation:**

```sql
-- ========================================
-- PARTITION AUDIT_LOGS BY MONTH
-- ========================================

-- Step 1: Create new partitioned table
CREATE TABLE audit_logs_new (
    id SERIAL,
    usuario VARCHAR(255),
    modulo VARCHAR(255),
    accion VARCHAR(255),
    detalles TEXT,
    created_at TIMESTAMP NOT NULL,
    nivel_log VARCHAR(50),
    estado VARCHAR(50),
    PRIMARY KEY (id, created_at)  -- Include partition key in PK
) PARTITION BY RANGE (created_at);

-- Step 2: Create partitions for each month (last 12 months + future)
CREATE TABLE audit_logs_2025_01 PARTITION OF audit_logs_new
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

CREATE TABLE audit_logs_2025_02 PARTITION OF audit_logs_new
    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');

-- ... (create for each month)

CREATE TABLE audit_logs_2026_02 PARTITION OF audit_logs_new
    FOR VALUES FROM ('2026-02-01') TO ('2026-03-01');

CREATE TABLE audit_logs_2026_03 PARTITION OF audit_logs_new
    FOR VALUES FROM ('2026-03-01') TO ('2026-04-01');

-- Future partitions (template for automation)
CREATE TABLE audit_logs_future PARTITION OF audit_logs_new
    DEFAULT;  -- Catches any date not in specific partition

-- Step 3: Copy data from old table
INSERT INTO audit_logs_new SELECT * FROM audit_logs;

-- Step 4: Verify data integrity
SELECT COUNT(*) FROM audit_logs;     -- Original count
SELECT COUNT(*) FROM audit_logs_new; -- Should match

-- Step 5: Rename tables (DURING MAINTENANCE WINDOW)
BEGIN;
    ALTER TABLE audit_logs RENAME TO audit_logs_old;
    ALTER TABLE audit_logs_new RENAME TO audit_logs;

    -- Update sequences
    SELECT setval('audit_logs_id_seq', (SELECT MAX(id) FROM audit_logs));
COMMIT;

-- Step 6: Drop old table (AFTER VERIFICATION - wait 1 week)
-- DROP TABLE audit_logs_old;

-- Step 7: Create scheduled job to create future partitions
-- (Run monthly)
CREATE OR REPLACE FUNCTION create_next_month_partition()
RETURNS void AS $$
DECLARE
    next_month DATE := date_trunc('month', CURRENT_DATE + INTERVAL '2 months');
    partition_name TEXT := 'audit_logs_' || to_char(next_month, 'YYYY_MM');
    start_date TEXT := to_char(next_month, 'YYYY-MM-DD');
    end_date TEXT := to_char(next_month + INTERVAL '1 month', 'YYYY-MM-DD');
BEGIN
    EXECUTE format(
        'CREATE TABLE IF NOT EXISTS %I PARTITION OF audit_logs FOR VALUES FROM (%L) TO (%L)',
        partition_name, start_date, end_date
    );
    RAISE NOTICE 'Created partition % for range % to %', partition_name, start_date, end_date;
END;
$$ LANGUAGE plpgsql;

-- Schedule to run on 1st of each month
-- (Add to crontab or use pg_cron extension)
```

**Benefits of Partitioning:**

1. **Query Performance:**
   ```sql
   -- Query with date range (only scans 1 partition)
   SELECT * FROM audit_logs
   WHERE created_at BETWEEN '2026-02-01' AND '2026-02-28';
   -- Scans only audit_logs_2026_02 (not all 12+ partitions)
   ```

2. **Easy Archival:**
   ```sql
   -- Archive old partition (instant, no DELETE required)
   ALTER TABLE audit_logs DETACH PARTITION audit_logs_2025_01;

   -- Export to cold storage
   pg_dump -t audit_logs_2025_01 > archive_2025_01.sql

   -- Drop partition to reclaim space
   DROP TABLE audit_logs_2025_01;
   ```

3. **Maintenance Efficiency:**
   ```sql
   -- VACUUM only specific partition (faster)
   VACUUM ANALYZE audit_logs_2026_02;

   -- REINDEX only specific partition
   REINDEX TABLE audit_logs_2026_02;
   ```

**Update Spring Boot Entities:**

```java
// AuditLog.java (NO CHANGES REQUIRED)
// Partitioning is transparent to application

@Entity
@Table(name = "audit_logs")
public class AuditLog {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    // ... (no changes)

    @Column(name = "created_at")
    private LocalDateTime createdAt;
}

// Queries work exactly the same:
// auditLogRepository.findByCreatedAtBetween(start, end);
// PostgreSQL automatically routes to correct partition
```

**Impact:**
- Query performance: +30-50% for date-range queries
- Maintenance efficiency: VACUUM/REINDEX 10x faster (per partition)
- Space reclamation: Instant (detach partition vs DELETE millions of rows)
- **NO application code changes required**
- **NO distributed transaction issues**
- **Single database, native PostgreSQL feature**

#### Improvement 2: Read Replica (Simple CQRS)

**Purpose:** Offload reporting/analytics queries WITHOUT complex architecture

**Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   APPLICATION LAYER                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Write Operations (Transactional):                          â”‚
â”‚  - atenderPaciente()                                        â”‚
â”‚  - createUser()                                             â”‚
â”‚  - assignMedico()                                           â”‚
â”‚                    â†“                                        â”‚
â”‚            PRIMARY DATABASE (10.0.89.241)                   â”‚
â”‚                    â”‚                                        â”‚
â”‚                    â”‚ PostgreSQL Streaming Replication       â”‚
â”‚                    â†“                                        â”‚
â”‚            READ REPLICA (10.0.89.242 - NEW)                 â”‚
â”‚                    â†‘                                        â”‚
â”‚  Read Operations (Analytics):                               â”‚
â”‚  - estadisticas()                                           â”‚
â”‚  - reporteMensual()                                         â”‚
â”‚  - dashboardKPIs()                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Setup PostgreSQL Streaming Replication:**

```bash
# On PRIMARY (10.0.89.241):
# 1. Edit postgresql.conf
wal_level = replica
max_wal_senders = 3
max_replication_slots = 3
hot_standby = on

# 2. Create replication user
psql -U postgres -d maestro_cenate <<EOF
CREATE ROLE replicator WITH REPLICATION PASSWORD 'StrongPassword123' LOGIN;
EOF

# 3. Edit pg_hba.conf
echo "host replication replicator 10.0.89.242/32 md5" >> pg_hba.conf

# 4. Restart PostgreSQL
docker restart postgres_cenate

# On REPLICA (10.0.89.242 - NEW SERVER):
# 1. Deploy PostgreSQL container
docker run -d \
    --name postgres_cenate_replica \
    -e POSTGRES_PASSWORD=StrongPassword123 \
    -v replica-data:/var/lib/postgresql/data \
    -p 5432:5432 \
    postgres:16

# 2. Create base backup from primary
docker exec postgres_cenate_replica pg_basebackup \
    -h 10.0.89.241 \
    -U replicator \
    -D /var/lib/postgresql/data \
    -P -R

# 3. Start replica (will automatically sync)
docker start postgres_cenate_replica

# 4. Verify replication lag
psql -h 10.0.89.242 -U postgres -d maestro_cenate <<EOF
SELECT pg_last_wal_receive_lsn() - pg_last_wal_replay_lsn() AS replication_lag;
EOF
# Expected: 0-100MB (few seconds lag)
```

**Update Spring Boot Configuration:**

```yaml
# application.yml
spring:
  datasource:
    primary:
      jdbc-url: jdbc:postgresql://10.0.89.241:5432/maestro_cenate
      username: postgres
      password: ${DB_PASSWORD}
      hikari:
        maximum-pool-size: 100

    replica:
      jdbc-url: jdbc:postgresql://10.0.89.242:5432/maestro_cenate
      username: postgres
      password: ${DB_PASSWORD}
      hikari:
        maximum-pool-size: 50
        read-only: true  # Important: prevent writes
```

**Java Configuration:**

```java
@Configuration
public class DataSourceConfig {

    @Primary
    @Bean(name = "primaryDataSource")
    @ConfigurationProperties("spring.datasource.primary")
    public DataSource primaryDataSource() {
        return DataSourceBuilder.create().build();
    }

    @Bean(name = "replicaDataSource")
    @ConfigurationProperties("spring.datasource.replica")
    public DataSource replicaDataSource() {
        return DataSourceBuilder.create().build();
    }

    @Primary
    @Bean(name = "primaryEntityManagerFactory")
    public LocalContainerEntityManagerFactoryBean primaryEntityManagerFactory(
            @Qualifier("primaryDataSource") DataSource dataSource) {
        // Standard configuration
    }

    @Bean(name = "replicaEntityManagerFactory")
    public LocalContainerEntityManagerFactoryBean replicaEntityManagerFactory(
            @Qualifier("replicaDataSource") DataSource dataSource) {
        // Read-only configuration
    }
}
```

**Service Layer Usage:**

```java
@Service
public class EstadisticasService {

    @PersistenceContext(unitName = "replica")  // Use replica for read-only
    private EntityManager replicaEntityManager;

    @PersistenceContext(unitName = "primary")  // Use primary for writes
    private EntityManager primaryEntityManager;

    // Read-only query (uses replica)
    public EstadisticasDTO obtenerEstadisticasMensuales() {
        Query query = replicaEntityManager.createQuery(
            "SELECT COUNT(s) FROM SolicitudBolsa s WHERE s.fechaCreacion > :fechaInicio"
        );
        // ... (heavy analytics query on replica, doesn't affect primary)
    }

    // Write operation (uses primary)
    @Transactional(transactionManager = "primaryTransactionManager")
    public void crearSolicitud(SolicitudDTO dto) {
        // ... (writes to primary)
    }
}
```

**Benefits:**

1. **Performance Isolation:**
   - Heavy analytics queries don't slow down primary database
   - Primary database dedicated to OLTP (transactional operations)
   - Replica handles OLAP (analytical queries)

2. **No Distributed Transactions:**
   - All writes still go to single primary database
   - Strong consistency for transactional data
   - Eventual consistency only for analytics (acceptable 1-5 second lag)

3. **Simple Failover:**
   - If primary fails, promote replica to primary (1 command)
   - Minimal downtime (RTO < 5 minutes)

4. **Low Complexity:**
   - No saga patterns
   - No synchronization jobs
   - PostgreSQL native feature (battle-tested)

**Impact:**
- Offload 50-80% of SELECT queries to replica
- Primary database CPU/IO usage decreases 40-60%
- Analytics queries don't block transactional operations
- **NO code changes for transactional operations**
- **NO distributed transaction complexity**

**Recommendation:** Implement Read Replica BEFORE considering separate databases for different domains.

---

### **Phase 3: Only If Justified - Multi-Database Split**

**Timeline:** 2-3 months
**Effort:** Very High
**Risk:** High

**IMPORTANT: Only proceed with this phase IF:**

1. âœ… Phase 0 diagnostics completed
2. âœ… Phase 1 quick wins implemented (audit rotation, index rebuild, incremental backups)
3. âœ… Phase 2 structural improvements applied (partitioning, read replica)
4. âœ… Performance still inadequate despite above optimizations
5. âœ… Specific bottleneck identified (e.g., "audit_logs table causes 80% of slow queries")
6. âœ… Measured performance metrics show need (e.g., "95th percentile latency > 5 seconds")
7. âœ… Business justification documented (e.g., "user complaints, SLA violations")

**Architectural Decision Matrix:**

```
IF all Phase 1 + Phase 2 optimizations applied AND:
    - Single table > 50GB (even after archival)
    - Sustained query load > 5000 QPS
    - 95th percentile latency > 5 seconds
    - Clear domain boundaries identified (medical vs audit)
    - Team capacity for distributed system management (2+ DBAs)
THEN:
    CONSIDER multi-database split
ELSE:
    DO NOT implement second database
```

**Recommended Split Strategy (if criteria met):**

**Option: Domain-Based Split (Audit Segregation)**

```
DATABASE 1: maestro_cenate_core (Primary)
â”œâ”€â”€ Domain: Medical Operations + Patient Data
â”œâ”€â”€ Tables:
â”‚   â”œâ”€â”€ asegurados (patient master)
â”‚   â”œâ”€â”€ dim_solicitud_bolsa (patient assignments)
â”‚   â”œâ”€â”€ solicitud_cita (appointments)
â”‚   â”œâ”€â”€ receta (prescriptions)
â”‚   â”œâ”€â”€ interconsulta (referrals)
â”‚   â”œâ”€â”€ personal (staff)
â”‚   â”œâ”€â”€ ipress (institutions)
â”‚   â”œâ”€â”€ usuarios (users)
â”‚   â””â”€â”€ roles/permisos (authorization)
â””â”€â”€ Characteristics:
    - Transactional (OLTP)
    - Strong consistency required
    - Critical business operations
    - Backup: Incremental daily

DATABASE 2: maestro_cenate_audit (Secondary)
â”œâ”€â”€ Domain: Audit & Logging
â”œâ”€â”€ Tables:
â”‚   â”œâ”€â”€ audit_logs (system audit)
â”‚   â”œâ”€â”€ email_audit_log (email tracking)
â”‚   â”œâ”€â”€ audit_asegurados_deletes (delete tracking)
â”‚   â””â”€â”€ performance_logs (metrics)
â””â”€â”€ Characteristics:
    - Append-only (no updates/deletes)
    - Eventual consistency acceptable
    - Non-blocking (failures don't affect core)
    - Backup: Weekly full + purge old data
```

**Rationale:**

1. **Audit logs naturally asynchronous:**
   - If audit write fails, core operation still succeeds
   - Eventual consistency acceptable for audit trail
   - No foreign key dependencies to core tables

2. **Different backup/retention requirements:**
   - Core DB: Full integrity, incremental backups
   - Audit DB: Shorter retention (90 days), less frequent backups

3. **Performance isolation:**
   - Heavy audit writes don't lock core tables
   - Audit queries (reporting) don't slow down transactions

**Implementation Strategy:**

```java
// AuditLogService.java (Enhanced for separate DB)
@Service
@Slf4j
public class AuditLogService {

    @PersistenceContext(unitName = "audit")  // Separate datasource
    private EntityManager auditEntityManager;

    @Async  // Non-blocking
    @Transactional(transactionManager = "auditTransactionManager", propagation = Propagation.REQUIRES_NEW)
    public void logAction(AuditLogDTO auditLog) {
        try {
            AuditLog entity = new AuditLog();
            // ... (map DTO to entity)
            auditEntityManager.persist(entity);

        } catch (Exception e) {
            // CRITICAL: Audit failure must NOT fail core operation
            log.error("Failed to write audit log (non-blocking): {}", e.getMessage());
            // Optionally: write to fallback (file, message queue)
        }
    }
}
```

**Key Principles:**

1. **Audit writes are fire-and-forget:**
   - Core transaction commits independently
   - Audit write happens asynchronously
   - If audit fails, core operation still succeeds

2. **No cross-DB joins:**
   - Audit logs store denormalized data (user name, not just user ID)
   - No foreign keys from audit DB to core DB

3. **Fallback mechanism:**
   ```java
   @Component
   public class AuditLogFallbackWriter {

       @EventListener(AuditLogFailureEvent.class)
       public void handleAuditFailure(AuditLogFailureEvent event) {
           // Fallback: write to file
           Path fallbackFile = Paths.get("/var/log/cenate/audit_fallback.log");
           String logEntry = String.format("[%s] %s - %s\n",
                   event.getTimestamp(), event.getUsuario(), event.getAccion());
           Files.write(fallbackFile, logEntry.getBytes(), StandardOpenOption.APPEND);
       }
   }
   ```

**Testing Strategy:**

```java
@SpringBootTest
@Transactional(transactionManager = "primaryTransactionManager")
public class DistributedAuditTest {

    @Test
    public void testCoreOperationSucceedsEvenIfAuditFails() {
        // Simulate audit database failure
        simulateAuditDatabaseDown();

        // Core operation should still succeed
        SolicitudBolsa solicitud = solicitudService.crearSolicitud(dto);

        assertNotNull(solicitud.getId());  // Core operation successful

        // Verify audit fallback
        List<String> fallbackLogs = readFallbackLogFile();
        assertTrue(fallbackLogs.stream().anyMatch(log -> log.contains("crearSolicitud")));
    }

    @Test
    public void testAuditEventualConsistency() {
        // Create core record
        SolicitudBolsa solicitud = solicitudService.crearSolicitud(dto);

        // Wait for async audit write (eventual consistency)
        await().atMost(Duration.ofSeconds(5))
               .until(() -> auditLogRepository.findByEntityId(solicitud.getId()) != null);

        // Verify audit record created
        AuditLog auditLog = auditLogRepository.findByEntityId(solicitud.getId());
        assertEquals("CREATE", auditLog.getAccion());
    }
}
```

**Deployment Checklist:**

- [ ] Phase 0-2 completed and verified
- [ ] Business case approved (cost/benefit analysis)
- [ ] Team trained on distributed system debugging
- [ ] Monitoring enhanced (distributed tracing with Micrometer/Zipkin)
- [ ] Rollback plan documented
- [ ] Canary deployment strategy (10% â†’ 50% â†’ 100%)
- [ ] Load testing with production-like data
- [ ] Database failover procedures documented
- [ ] On-call rotation for 2+ DBAs

---

## 5. Long-Term Implications

### How This Change Affects Future Extensibility

#### Scenario 1: If Second Database is Implemented (Without Diagnosis)

**Negative Impacts:**

1. **Feature Development Velocity: -50%**
   ```
   Before (1 DB):
   - Add new endpoint: 1 day
   - Add new entity: 2 hours
   - Debug transaction issue: 30 minutes

   After (2 DBs):
   - Add new endpoint: 2-3 days (decide which DB, handle cross-DB queries)
   - Add new entity: 1 day (choose DB, update datasource routing)
   - Debug distributed transaction: 4-8 hours (multiple DBs, network, timing)
   ```

2. **Onboarding New Developers: +200% Time**
   ```
   Before:
   - Understand Spring Boot: 1 week
   - Understand CENATE domain: 1 week
   - First contribution: Week 3
   Total: 3 weeks

   After:
   - Understand Spring Boot: 1 week
   - Understand CENATE domain: 1 week
   - Understand multi-datasource config: 3 days
   - Understand saga patterns: 3 days
   - Understand which data lives where: 2 days
   - Debug first distributed transaction: 3 days
   - First contribution: Week 5-6
   Total: 5-6 weeks
   ```

3. **Technical Debt Accumulation:**
   ```
   Month 1: "Quick hack to sync data manually between DBs"
   Month 3: "Custom sync service with retry logic"
   Month 6: "Why is data inconsistent between DB1 and DB2?"
   Month 12: "Can we consolidate back to 1 DB? (Too risky now)"
   Year 2: "Team only knows the workarounds, not the design"
   ```

4. **Operational Burden:**
   ```
   Monitoring:
   - 1 DB: 1 Grafana dashboard, 5 metrics
   - 2 DBs: 3 Grafana dashboards, 15+ metrics, 2 alert rules

   Backups:
   - 1 DB: 1 backup job, 1 restore procedure
   - 2 DBs: 2 backup jobs, 2 restore procedures, sync verification

   Disaster Recovery:
   - 1 DB: RTO 15 min, 1 runbook
   - 2 DBs: RTO 45 min, 3 runbooks (DB1, DB2, sync restoration)
   ```

#### Scenario 2: If Diagnostic-First Approach is Followed

**Positive Impacts:**

1. **Targeted Optimizations:**
   ```
   Week 1: Identify audit_logs as 40% of database
   Week 2: Implement rotation, reclaim 35GB
   Week 3: REINDEX bloated indices, improve query speed 40%
   Week 4: Incremental backups, reduce storage 90%

   Result: 98GB â†’ 50GB, queries 50% faster, same architecture
   ```

2. **Knowledge Building:**
   ```
   Team learns:
   - PostgreSQL performance tuning (transferable skill)
   - Proper backup strategies (industry best practice)
   - Table partitioning (native feature, simple)
   - Query optimization (EXPLAIN ANALYZE)

   Instead of:
   - Distributed transactions (rarely needed, complex)
   - Cross-DB query patterns (error-prone)
   - Data synchronization (source of bugs)
   ```

3. **Future Flexibility:**
   ```
   With clean, optimized single DB:
   - Can add read replicas easily (1 config change)
   - Can partition tables by time (PostgreSQL native)
   - Can archive historical data (simple SQL)
   - Can scale vertically (more CPU/RAM to 1 server)

   With premature multi-DB:
   - Locked into complex architecture
   - Hard to refactor (data split across DBs)
   - Can't easily consolidate (business logic depends on split)
   ```

### Will This Make Future Changes Harder or Easier?

#### Analysis: Second Database (Harder)

**Future Change 1: Add new medical workflow (e.g., "Telemedicina Video")**

```
Current Complexity (1 DB): LOW
1. Create new entity VideollamadaSession
2. Add service VideollamadaService
3. Add endpoint /api/videollamada
4. Deploy
Time: 2 days

Multi-DB Complexity: HIGH
1. Decide: Which DB? (medical operations â†’ DB1)
2. But: Audit logs go to DB2 (cross-DB write)
3. And: If session references solicitud_cita (DB1) + stores recording metadata (DB1 or DB2?)
4. Implement distributed transaction or saga
5. Add cross-DB query for reports (join videollamada + patient data)
6. Handle edge cases (DB2 down during videollamada creation)
7. Deploy with rollback strategy
Time: 1 week
```

**Future Change 2: GDPR compliance - anonymize patient data after 7 years**

```
Current Complexity (1 DB): MEDIUM
1. Identify tables with patient data (asegurados, dim_solicitud_bolsa, etc.)
2. Write SQL script to anonymize (UPDATE asegurados SET paciente='ANONYMIZED'...)
3. Add scheduled job
4. Test on copy of production
5. Deploy
Time: 1 week

Multi-DB Complexity: VERY HIGH
1. Identify tables across 2 databases
2. Write 2 separate anonymization scripts (can't join across DBs)
3. Ensure atomicity (both DBs anonymize or neither)
4. Handle case: DB1 succeeds, DB2 fails (partial anonymization â†’ legal risk)
5. Implement distributed transaction coordinator
6. Test on 2 copies of production
7. Deploy with complex rollback (2 DBs)
Time: 3 weeks
```

**Future Change 3: Migrate to Kubernetes (cloud deployment)**

```
Current Complexity (1 DB): MEDIUM
1. Containerize Spring Boot app (Dockerfile)
2. Create Kubernetes manifests (Deployment, Service)
3. Configure PostgreSQL StatefulSet
4. Setup persistent volumes
5. Deploy
Time: 1 week

Multi-DB Complexity: VERY HIGH
1. Containerize Spring Boot app (2 datasources config)
2. Create Kubernetes manifests (app needs to reach 2 DBs)
3. Configure 2 PostgreSQL StatefulSets
4. Setup persistent volumes (2x)
5. Configure network policies (app can reach DB1 + DB2)
6. Handle DB1 in different availability zone than DB2 (network latency)
7. Implement distributed health checks
8. Deploy with blue-green strategy (complex with 2 DBs)
Time: 3-4 weeks
```

#### Analysis: Diagnostic-First Approach (Easier)

**Future Change 1: Add new medical workflow**
- Same as current (1 DB)
- Time: 2 days

**Future Change 2: GDPR compliance**
- Same as current (1 DB)
- Time: 1 week

**Future Change 3: Kubernetes migration**
- Same as current (1 DB)
- Time: 1 week

**Conclusion:** Diagnostic-first approach makes ALL future changes easier.

### Scalability Implications

#### Database Scaling Options (Ranked by Complexity)

```
1. VERTICAL SCALING (Simplest)
   - Current: 1 server, 4 CPU, 8GB RAM
   - Upgrade: 1 server, 16 CPU, 64GB RAM
   - Cost: $200/month â†’ $800/month
   - Downtime: 15 minutes
   - Complexity: LOW (1 config change)
   - Capacity: 10x current load
   - Recommended: TRY FIRST

2. READ REPLICAS (Simple)
   - Current: 1 primary DB
   - Add: 1-3 read replicas
   - Cost: +$200/month per replica
   - Downtime: ZERO (add without stopping primary)
   - Complexity: LOW (Spring Boot @Transactional(readOnly=true))
   - Capacity: Read capacity 5-10x
   - Recommended: SECOND STEP

3. TABLE PARTITIONING (Medium)
   - Current: Single tables
   - Add: Partitions by date/id range
   - Cost: $0 (same hardware)
   - Downtime: 1-2 hours (migration)
   - Complexity: MEDIUM (PostgreSQL native, transparent to app)
   - Capacity: Query performance 2-5x
   - Recommended: THIRD STEP

4. CACHING LAYER (Medium)
   - Current: No cache
   - Add: Redis/Memcached
   - Cost: +$100/month
   - Downtime: ZERO
   - Complexity: MEDIUM (cache invalidation logic)
   - Capacity: Read latency 10-100x faster
   - Recommended: FOR HIGH-READ WORKLOADS

5. CONNECTION POOLING (Already Done)
   - Current: HikariCP pool = 100 connections
   - Upgrade: PgBouncer (external pooler)
   - Cost: $0
   - Complexity: LOW
   - Capacity: 1000+ concurrent connections
   - Status: ALREADY OPTIMIZED (v1.37.3)

6. SHARDING (Complex)
   - Current: 1 database
   - Add: Shard by institution (IPRESS_001 â†’ DB1, IPRESS_002 â†’ DB2, etc.)
   - Cost: +$400/month (2+ servers)
   - Downtime: 1 day (migration)
   - Complexity: HIGH (application-level routing)
   - Capacity: Linear scaling (add shard = add capacity)
   - Recommended: ONLY IF >100M RECORDS

7. MULTI-DATABASE BY DOMAIN (Very Complex)
   - Current: 1 database
   - Add: Split by domain (medical/audit/admin)
   - Cost: +$400-800/month (2-3 servers)
   - Downtime: 1 week (migration + testing)
   - Complexity: VERY HIGH (distributed transactions, saga patterns)
   - Capacity: Depends on split (may not improve if wrong split)
   - Recommended: LAST RESORT, ONLY IF JUSTIFIED BY CLEAR BOTTLENECK
```

#### Capacity Planning (Current System)

```
Current State:
- Database: 98GB (suspected 50GB waste)
- Patients: 7,973 active (5.1M total asegurados)
- Daily operations: ~500-1000 atenciones
- Concurrent users: ~50-100 (doctors, coordinators)

After Phase 1-2 Optimizations (Projected):
- Database: 40-50GB (50% reduction)
- Query performance: 2-5x faster (indexes + partitioning)
- Backup storage: 90% reduction (incremental backups)
- Concurrent users supported: 500-1000 (read replicas + vertical scaling)

Scaling Capacity (Without Second Database):
- Vertical scaling: 10x current load (16 CPU, 64GB RAM server = $800/month)
- Read replicas: +5x read capacity per replica
- Partitioning: +2-5x query performance
- Caching: 10-100x faster reads

Total Capacity: 50-100x current load
Cost: ~$1500/month (vs $400-800/month for second DB with HIGHER complexity)
```

**Conclusion:** Current architecture can scale to 50-100x current load WITHOUT second database.

#### When Would Second Database Actually Be Needed?

```
Threshold Metrics (ALL must be true):

1. VOLUME:
   - Total records > 100M
   - Single table > 1TB (even after partitioning)
   - Database > 500GB (after archival, optimization)

2. LOAD:
   - Sustained query load > 10,000 QPS
   - Concurrent connections > 1,000
   - 95th percentile latency > 5 seconds (after optimization)

3. ARCHITECTURAL:
   - Clear bounded contexts (medical vs audit)
   - No cross-domain transactions required
   - Team has distributed systems expertise (2+ DBAs)

4. BUSINESS:
   - SLA violations documented
   - User complaints tracked
   - Revenue impact measured
   - Budget approved ($5,000+/month additional cost)

CENATE Current Status:
- Volume: 7,973 active patients âŒ (10,000x below threshold)
- Load: ~100 concurrent users âŒ (10x below threshold)
- Architectural: Cross-domain transactions exist âŒ (AtenderPacienteService)
- Business: No documented SLA violations âŒ

Conclusion: NOT JUSTIFIED
```

### Maintenance Burden

#### Comparison: Maintenance Effort

**Single Database (Optimized):**

```
Weekly Tasks:
- Review backup success (10 min)
- Check slow query log (15 min)
- Monitor disk space (5 min)
Total: 30 min/week

Monthly Tasks:
- Review and archive old audit logs (1 hour)
- REINDEX bloated indices (1 hour, automated)
- Verify recovery procedures (2 hours)
Total: 4 hours/month

Quarterly Tasks:
- Performance review (4 hours)
- Backup restoration test (4 hours)
- Capacity planning (2 hours)
Total: 10 hours/quarter

Annual Effort: 30 + (4Ã—12) + (10Ã—4) = 30 + 48 + 40 = 118 hours/year
FTE: 0.06 (6% of 1 DBA)
```

**Multi-Database (Separate DBs):**

```
Weekly Tasks:
- Review backup success (2 DBs) (20 min)
- Check slow query log (2 DBs) (30 min)
- Monitor disk space (2 DBs) (10 min)
- Verify cross-DB sync status (30 min) â† NEW
- Check replication lag (if using sync) (15 min) â† NEW
Total: 105 min/week

Monthly Tasks:
- Review and archive old audit logs (2 hours)
- REINDEX bloated indices (2 DBs) (2 hours)
- Verify recovery procedures (2 DBs + sync) (6 hours) â† 3x longer
- Debug sync failures (4 hours) â† NEW
Total: 14 hours/month

Quarterly Tasks:
- Performance review (2 DBs) (8 hours)
- Backup restoration test (2 DBs + sync) (12 hours) â† 3x longer
- Capacity planning (2 DBs) (4 hours)
- Review cross-DB query patterns (4 hours) â† NEW
Total: 28 hours/quarter

Annual Effort: 105 + (14Ã—12) + (28Ã—4) = 105 + 168 + 112 = 385 hours/year
FTE: 0.19 (19% of 1 DBA)

Increase: 385 - 118 = 267 hours/year (+226%)
```

**Hidden Maintenance Costs (Multi-Database):**

```
Incident Response:
- Single DB: 1 DBA investigates
- Multi-DB: 1 DBA investigates DB1, 1 DBA investigates DB2, 1 Dev investigates sync logic
  â†’ 3x longer MTTR (Mean Time To Recover)

Documentation:
- Single DB: 1 runbook
- Multi-DB: 4 runbooks (DB1 failure, DB2 failure, sync failure, both DBs failure)

Training:
- Single DB: 1 week onboarding for new DBA
- Multi-DB: 3 weeks (distributed systems, cross-DB debugging, saga patterns)

Monitoring:
- Single DB: 1 Grafana dashboard
- Multi-DB: 3 dashboards + distributed tracing (Zipkin/Jaeger)
  â†’ +$200/month for monitoring tools
```

#### Technical Debt Growth Rate

**Single Database (Optimized):**
```
Technical Debt Growth: LINEAR
- Audit logs grow predictably
- Archival strategy in place
- Performance degrades slowly (predictable)

Debt Servicing: QUARTERLY
- Review query performance
- Add indices as needed
- Archive old data
```

**Multi-Database (Premature):**
```
Technical Debt Growth: EXPONENTIAL
- Sync failures accumulate
- Workarounds multiply (developers bypass proper sync)
- Cross-DB queries add N+1 query problems

Debt Servicing: CONTINUOUS
- Daily sync failure triage
- Weekly cross-DB query optimization
- Monthly refactoring to reduce cross-DB calls

Compounding Effect:
Year 1: 10 workarounds
Year 2: 30 workarounds (3x)
Year 3: 100 workarounds (10x original)
Year 5: System unmaintainable, requires full rewrite
```

**Real-World Example (From Other Projects):**

```
Company A: Premature Microservices Split (Similar to Premature DB Split)
- Year 1: "We need to scale, let's split into 20 microservices"
- Year 2: 50% of engineering time spent on inter-service debugging
- Year 3: Feature velocity down 70% (complexity overwhelming)
- Year 4: CTO hired "Back to the Monolith" migration team
- Year 5: Consolidated back to 3 services
- Total cost: $2M wasted + 4 years lost

Company B: Diagnostic-First Approach
- Year 1: "We have performance issues, let's diagnose"
- Diagnosis: Single slow query causing 80% of issues
- Fix: Add 1 index (1 line of SQL)
- Result: 95th percentile latency 200ms â†’ 20ms
- Cost: $0 + 1 day engineering time
- Outcome: System handled 10x load for 5 more years
```

---

## 6. Verification Checklist

### Before Making Any Architectural Changes

- [ ] **Phase 0: Diagnosis MANDATORY**
  - [ ] Run `spec/database/diagnostic_queries.sql`
  - [ ] Identify top 5 space consumers
  - [ ] Document current performance metrics (baseline)
  - [ ] Identify actual bottleneck (query? write? storage?)
  - [ ] Calculate expected improvement per fix

- [ ] **Verify Against Documentation**
  - [ ] Review `spec/database/08_plan_backup_protecciones_completo.md` (backup strategy)
  - [ ] Review `spec/backend/15_recita_interconsulta_v1.47.md` (transactional workflows)
  - [ ] Review `spec/architecture/01_flujo_atenciones_completo.md` (service boundaries)
  - [ ] Review `spec/backend/14_sincronizacion_atendido/README.md` (synchronization patterns)

- [ ] **Stakeholder Alignment**
  - [ ] Present diagnostic findings to team
  - [ ] Get approval for recommended approach (NOT second DB)
  - [ ] Estimate timeline and effort for Phase 1-2 optimizations
  - [ ] Communicate "second database NOT needed" decision with evidence

### If Proceeding with Phase 1 (Quick Wins)

- [ ] **Audit Log Rotation**
  - [ ] Test rotation script on staging
  - [ ] Verify CSV export works
  - [ ] Verify DELETE removes old records
  - [ ] Verify VACUUM reclaims space
  - [ ] Monitor for 1 week before production

- [ ] **Index Rebuild**
  - [ ] Schedule during maintenance window
  - [ ] Use REINDEX CONCURRENTLY (online operation)
  - [ ] Verify space reclaimed
  - [ ] Measure query performance improvement
  - [ ] Document before/after metrics

- [ ] **Incremental Backups**
  - [ ] Configure WAL archiving in postgresql.conf
  - [ ] Test base backup + WAL restore
  - [ ] Verify recovery procedure works
  - [ ] Update runbooks
  - [ ] Train ops team on new backup strategy

### If Proceeding with Phase 2 (Structural Improvements)

- [ ] **Table Partitioning**
  - [ ] Create partitioned table structure
  - [ ] Test data migration on copy of production
  - [ ] Verify application code unaffected
  - [ ] Plan rollback strategy
  - [ ] Execute during maintenance window

- [ ] **Read Replica**
  - [ ] Setup replication between primary and replica
  - [ ] Verify replication lag < 5 seconds
  - [ ] Update Spring Boot datasource configuration
  - [ ] Test read-only queries on replica
  - [ ] Monitor replication health

### If (Against Recommendation) Proceeding with Multi-Database

- [ ] **âš ï¸ WARNING: This is NOT recommended. Only proceed if:**
  - [ ] ALL Phase 0-2 completed
  - [ ] Performance STILL inadequate
  - [ ] Business case approved (cost/benefit analysis)
  - [ ] Team trained on distributed systems (2+ DBAs available)
  - [ ] Distributed tracing infrastructure in place (Zipkin/Jaeger)

- [ ] **Architecture Review**
  - [ ] Document which tables go to which DB (boundary analysis)
  - [ ] Identify all cross-DB transactions (require saga patterns)
  - [ ] Design saga orchestration service
  - [ ] Design compensation logic (rollback distributed transactions)
  - [ ] Update all sequence diagrams

- [ ] **Code Changes**
  - [ ] Implement multi-datasource configuration
  - [ ] Update all services with datasource routing
  - [ ] Implement saga coordinator service
  - [ ] Add distributed transaction tests
  - [ ] Update error handling (handle partial failures)

- [ ] **Testing**
  - [ ] Unit tests for saga patterns (100% coverage)
  - [ ] Integration tests with 2 databases
  - [ ] Chaos testing (DB1 fails, DB2 fails, network partition)
  - [ ] Load testing (production-like volume)
  - [ ] Disaster recovery drill (restore both DBs + verify sync)

- [ ] **Deployment**
  - [ ] Blue-green deployment strategy
  - [ ] Canary deployment (10% â†’ 50% â†’ 100%)
  - [ ] Rollback plan documented and tested
  - [ ] 24/7 on-call for first 2 weeks
  - [ ] Post-mortem scheduled (2 weeks after deployment)

---

## 7. References

### Relevant CENATE Documentation

1. **Architecture:**
   - [`spec/architecture/01_flujo_atenciones_completo.md`](/Users/styp/Documents/CENATE/Chatbot/API_Springboot/mini_proyecto_cenate/spec/architecture/01_flujo_atenciones_completo.md)
     - Documents transactional workflows (atender paciente)
     - Shows service boundaries and data flow

2. **Backend Services:**
   - [`spec/backend/15_recita_interconsulta_v1.47.md`](/Users/styp/Documents/CENATE/Chatbot/API_Springboot/mini_proyecto_cenate/spec/backend/15_recita_interconsulta_v1.47.md)
     - AtenderPacienteService.atenderPaciente() - MUST be atomic
   - [`spec/backend/14_sincronizacion_atendido/README.md`](/Users/styp/Documents/CENATE/Chatbot/API_Springboot/mini_proyecto_cenate/spec/backend/14_sincronizacion_atendido/README.md)
     - Synchronization patterns (v1.44.0+)

3. **Database:**
   - [`spec/database/08_plan_backup_protecciones_completo.md`](/Users/styp/Documents/CENATE/Chatbot/API_Springboot/mini_proyecto_cenate/spec/database/08_plan_backup_protecciones_completo.md)
     - Current backup strategy (FULL backups, not incremental)
   - [`spec/database/diagnostic_queries.sql`](/Users/styp/Documents/CENATE/Chatbot/API_Springboot/mini_proyecto_cenate/spec/database/diagnostic_queries.sql)
     - Comprehensive diagnostic queries (MUST RUN FIRST)

4. **Performance:**
   - [`spec/backend/10_performance_monitoring/README.md`](/Users/styp/Documents/CENATE/Chatbot/API_Springboot/mini_proyecto_cenate/spec/backend/10_performance_monitoring/README.md)
     - Performance monitoring setup (HikariCP, Actuator)

### External Resources

1. **PostgreSQL Official Documentation:**
   - [Table Partitioning](https://www.postgresql.org/docs/current/ddl-partitioning.html)
   - [Streaming Replication](https://www.postgresql.org/docs/current/warm-standby.html)
   - [VACUUM and REINDEX](https://www.postgresql.org/docs/current/routine-vacuuming.html)

2. **Spring Boot Multi-Datasource:**
   - [Spring Boot Multiple Datasources](https://spring.io/guides/gs/accessing-data-jpa/)
   - [Distributed Transactions with JTA](https://www.baeldung.com/java-atomikos)

3. **Architectural Patterns:**
   - [Saga Pattern (Chris Richardson)](https://microservices.io/patterns/data/saga.html)
   - [CQRS Pattern](https://martinfowler.com/bliki/CQRS.html)
   - [Premature Optimization (Donald Knuth)](https://wiki.c2.com/?PrematureOptimization)

4. **Database Scaling:**
   - [PostgreSQL Performance Tuning](https://wiki.postgresql.org/wiki/Performance_Optimization)
   - [When to Shard PostgreSQL](https://www.citusdata.com/blog/2018/01/10/when-to-use-sharding/)

---

## 8. Conclusion

### Summary of Findings

1. **Premature Optimization:**
   - 98GB for 7,973 patients is ABNORMAL (12MB/patient vs 10KB expected)
   - Problem is likely operational (audit logs, index bloat, backup strategy)
   - NO evidence of performance bottleneck requiring second database

2. **Architectural Violations:**
   - AtenderPacienteService requires atomic transaction across multiple tables
   - Splitting dim_solicitud_bolsa and solicitud_cita breaks ACID guarantees
   - Multi-database WITHOUT bounded context analysis violates SRP

3. **Cost/Benefit Analysis:**
   ```
   Second Database Approach:
   - Cost: $5,000-10,000 engineering time + $400-800/month infrastructure
   - Benefit: UNKNOWN (no bottleneck identified)
   - Risk: HIGH (distributed transactions, data inconsistency)
   - Complexity: 10x increase

   Diagnostic-First Approach:
   - Cost: $2,000 engineering time (1 week)
   - Benefit: Reclaim 40-50GB + 50% faster queries
   - Risk: LOW (targeted optimizations)
   - Complexity: 0x increase (same architecture)
   ```

### Final Recommendation

**ğŸ”´ DO NOT IMPLEMENT SECOND DATABASE**

**Instead, follow this path:**

```
WEEK 1: Run diagnostics (spec/database/diagnostic_queries.sql)
        â†“
WEEK 2-3: Implement Phase 1 Quick Wins
          - Audit log rotation
          - Index rebuild
          - Incremental backups
        â†“
WEEK 4: Measure improvements
        â†“
MONTH 2: Implement Phase 2 Structural Improvements
         - Table partitioning
         - Read replica (if needed)
        â†“
MONTH 3: Monitor and verify
        â†“
DECISION POINT:
  IF performance adequate:
    â†’ DONE (stay with single DB)
  ELSE:
    â†’ Re-evaluate with NEW data
    â†’ Consider vertical scaling
    â†’ Consider caching layer
    â†’ ONLY as last resort: multi-database
```

**Rationale:**
- 90% of "database inefficiency" problems are operational, not architectural
- Jumping to second database solves WRONG problem
- Diagnostic-first approach is cheaper, faster, lower risk
- Single optimized database can handle 50-100x current load

### Approval Workflow

**Before proceeding with ANY database changes:**

1. [ ] Team Lead reviews diagnostic results
2. [ ] Database Administrator approves optimization plan
3. [ ] System Architect confirms no architectural violations
4. [ ] Business Owner approves timeline and downtime
5. [ ] Security Team reviews any permission changes

**Escalation:**
- If Phase 1-2 insufficient: Escalate to CTO with metrics
- If second database still proposed: Require external architecture review

---

**Document Prepared By:** Claude Code (Architecture Agent)
**Date:** 2026-02-06
**Status:** ğŸ”´ CRITICAL - DO NOT PROCEED WITHOUT DIAGNOSIS
**Next Review:** After diagnostic queries executed

---

**Contact:**
- Architecture Questions: Review with System Architect
- Database Questions: Consult Database Administrator
- Implementation: Refer to `/Users/styp/Documents/CENATE/Chatbot/API_Springboot/mini_proyecto_cenate/spec/database/diagnostic_queries.sql`

